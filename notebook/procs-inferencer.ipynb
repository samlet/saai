{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-03T03:56:40.801526Z",
     "start_time": "2020-03-03T03:56:40.114740Z"
    }
   },
   "outputs": [],
   "source": [
    "from sagas.nlu.utils import fix_sents\n",
    "import requests\n",
    "from sagas.conf.conf import cf\n",
    "\n",
    "def fix_data(data):\n",
    "    if 'engine' not in data:\n",
    "        data['engine'] = cf.engine(data['lang'])\n",
    "    data['sents']=fix_sents(data['sents'], data['lang'])\n",
    "    return data\n",
    "def parse(data):\n",
    "    if 'engine' not in data:\n",
    "        data['engine']=cf.engine(data['lang'])\n",
    "    engine=data['engine']\n",
    "    response = requests.post(f'{cf.servant(engine)}/verb_domains', json=data)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-03T03:56:40.828480Z",
     "start_time": "2020-03-03T03:56:40.804307Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'predicate',\n",
       "  'lemma': '混ぜる',\n",
       "  'index': 2,\n",
       "  'phonetic': 'まぜる',\n",
       "  'word': '混ぜた。',\n",
       "  'rel': 'D',\n",
       "  'governor': 0,\n",
       "  'pos': 'verb',\n",
       "  'domains': [['ヲ', 0, '水と', '水', ['水'], ['c_noun', 'x_n']],\n",
       "   ['ヲ', 1, 'しょうゆを', 'しょうゆ', ['醤油'], ['c_noun', 'x_n']]],\n",
       "  'stems': [],\n",
       "  'segments': [{'index': 4,\n",
       "    'upos': 'VERB',\n",
       "    'xpos': ['動詞', '*'],\n",
       "    'text': '混ぜた',\n",
       "    'lemmas': ['混ぜる', 'まぜる']},\n",
       "   {'index': 5, 'upos': 'PUNCT', 'xpos': ['特殊', '句点'], 'text': '。'}]}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# $ ses 'Ellos ya leyeron ese libro en la escuela.'\n",
    "# data={'lang': 'es', \"sents\": 'Ellos ya leyeron ese libro en la escuela.'}\n",
    "# $ se 'you are dead'\n",
    "# data={'lang': 'en', \"sents\": 'you are dead'}\n",
    "# $ sid 'Apa yang lebih murah?'  (What is cheaper?)\n",
    "# data={'lang': 'id', \"sents\": 'Apa yang lebih murah?'}\n",
    "# $ sid 'Berapa umur kamu?' (en=\"How old are you?\")\n",
    "# data={'lang': 'id', \"sents\": 'Berapa umur kamu?'}\n",
    "# $ sj '太陽は月に比べて大きいです。'\n",
    "# $ sj '予約を火曜日から木曜日に変えてもらった。'\n",
    "# data={'lang': 'ja', \"sents\": '予約を火曜日から木曜日に変えてもらった。'}\n",
    "# $ sj '水としょうゆを混ぜた。'\n",
    "data={'lang': 'ja', \"sents\": '水としょうゆを混ぜた。'}\n",
    "rs=parse(data)\n",
    "rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-03T03:56:40.839981Z",
     "start_time": "2020-03-03T03:56:40.831410Z"
    }
   },
   "outputs": [],
   "source": [
    "class AttrDict(dict):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(AttrDict, self).__init__(*args, **kwargs)\n",
    "        self.__dict__ = self\n",
    "def to_obj(m):\n",
    "    d = AttrDict()\n",
    "    d.update(m)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-03T03:59:54.015671Z",
     "start_time": "2020-03-03T03:59:53.868058Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicate\n",
      "\u001b[36m✁ lang.spec for ja.predicate -------------------------\u001b[0m\n",
      "\u001b[33m✖ (desc_fav) verb with pos is ins_interrogative(fav): False\u001b[0m\n",
      "\u001b[33m✖ (desc_fav_not) verb with pos is ins_interrogative(fav): False, pos is ins_checker{'has_lemma': 'ない'}: False\u001b[0m\n",
      "\u001b[33m✖ (desc_fav_neg) verb with pos is ins_interrogative(fav): False, pos is ins_checker{'negative': '_'}: False\u001b[0m\n",
      "\u001b[33m✖ (desc_attitude_type) verb with pos is specs_of(('tight',),*): False\u001b[0m\n",
      "\u001b[35m{'type': 'predicate', 'text': '混ぜた。', 'translit': '/hunzeta.', 'translate': 'mixed.', 'lang': 'ja', 'index': 2}\u001b[0m\n",
      "\u001b[36m{'name': 'ヲ', 'text': '水', 'translit': '/shui', 'translate': 'water', 'index': 0}\u001b[0m\n",
      "pat(5, name='predict_mix').verb(specsof('*', 'mix'), ヲ=kindof('water', '*')),\n",
      "* ---------------------------------------------\n",
      "[{'comments': ['mix.v.04',\n",
      "               'blend_in.v.02',\n",
      "               'admix.v.01',\n",
      "               'collocate.v.02',\n",
      "               'beat.v.10',\n",
      "               'shuffle.v.03',\n",
      "               'compound.v.05',\n",
      "               'stir.v.01'],\n",
      "  'indicator': '[predicate]',\n",
      "  'spec': 'mix',\n",
      "  'word': '混ぜた。/混ぜる'},\n",
      " {'comments': ['water.n.06', 'water.n.01', 'water.n.03'],\n",
      "  'indicator': 'ヲ',\n",
      "  'spec': 'water',\n",
      "  'word': '水と/水'},\n",
      " {'comments': ['soy_sauce.n.01'],\n",
      "  'indicator': 'ヲ',\n",
      "  'spec': 'soy_sauce',\n",
      "  'word': 'しょうゆを/しょうゆ'}]\n"
     ]
    }
   ],
   "source": [
    "from sagas.nlu.rules_meta import build_meta\n",
    "from sagas.nlu.rules_lang_spec import langspecs\n",
    "from sagas.nlu.sinkers import Sinkers\n",
    "from sagas.tool.misc import translit_chunk, display_synsets\n",
    "import sagas.tracker_fn as tc\n",
    "from sagas.nlu.utils import fix_sents, join_text\n",
    "import sagas\n",
    "from pprint import pprint\n",
    "from collections import Counter\n",
    "\n",
    "enable_verbose=True\n",
    "def proc_word(type_name, word, head, index, lang):\n",
    "    from sagas.nlu.google_translator import translate\n",
    "    res, _ = translate(word, source=lang, target=target_lang(lang),\n",
    "                       trans_verbose=False)\n",
    "    \n",
    "    # result=f\"[{type_name}]({word}{translit_chunk(word, lang)}) {res}{target}\"\n",
    "    result={'type':type_name, 'text':word, \n",
    "            'translit':translit_chunk(word, lang),\n",
    "            'translate': res,\n",
    "            'lang': lang,\n",
    "            'index': index,\n",
    "           }\n",
    "    if head!='':\n",
    "        res_t, _ = translate(head, source=lang, target=target_lang(lang),\n",
    "                           trans_verbose=False, options={'disable_correct'})\n",
    "        # target=f\" ⊙︿⊙ {res_t}({head})\"\n",
    "        result['head']=head\n",
    "        result['head_trans']=res_t\n",
    "    # tc.emp('magenta', result)\n",
    "    return result\n",
    "target_lang=lambda s: cf.optional('assist_lang', 'zh') if s=='en' else 'en'\n",
    "\n",
    "# def proc_children_column(partcol, textcol, idxcol, lang):\n",
    "def proc_children_column(df, lang):\n",
    "    from sagas.nlu.google_translator import translate\n",
    "    result=[]\n",
    "    # for id, (name, r) in enumerate(zip(partcol, textcol)):\n",
    "    rels=[]\n",
    "    for id, row in df.iterrows(): \n",
    "        # df['rel'], df['children'], df['index']\n",
    "        name, r, idx=row['rel'], row['children'], row['index']\n",
    "        if name in rels:\n",
    "            continue\n",
    "        else:\n",
    "            rels.append(name)\n",
    "        if name not in ('punct', 'head_root'):        \n",
    "            sent=join_text(r, lang)\n",
    "            res, _ = translate(sent, source=lang, target=target_lang(lang),\n",
    "                               trans_verbose=False, options={'disable_correct'})\n",
    "            # chunk=f\"{indent}[{name}]({sent}{translit_chunk(sent, lang)}) {res}\"\n",
    "            chunk={'name':name, 'text':sent, \n",
    "                   'translit':translit_chunk(sent, lang),\n",
    "                   'translate':res,\n",
    "                   'index':idx,\n",
    "                  }\n",
    "            result.append(chunk)\n",
    "            # tc.emp('cyan', chunk)\n",
    "    return result\n",
    "\n",
    "def induce_spec(el, pats, type_name):\n",
    "    # print(f\"{el.indicator} {el.word}: {el.spec}\")    \n",
    "    if el.indicator=='[verb]':\n",
    "        pats.append((3, f\"behaveof('{el.spec}', 'v')\"))\n",
    "    elif el.indicator=='[aux]':\n",
    "        # pats.append((3, f\"behaveof('{el.spec}', '*')\"))\n",
    "        pass\n",
    "    elif el.indicator=='[root]':\n",
    "        pats.append((3, f\"behaveof('{el.spec}', 'n')\"))\n",
    "    elif el.indicator=='[predicate]':\n",
    "        pats.append((3, f\"specsof('*', '{el.spec}')\"))\n",
    "    else:\n",
    "        if type_name in ('aux_domains', 'subj_domains') and el.indicator=='head':\n",
    "            pats.append((3, f\"behaveof('{el.spec}', '*')\"))\n",
    "        else:\n",
    "            pats.append((1, f\"{el.indicator}=kindof('{el.spec}', '*')\"))\n",
    "\n",
    "def induce_part(chunk, pats, type_name):\n",
    "    if enable_verbose:\n",
    "        tc.emp('cyan', chunk)\n",
    "    gen_map={'nsubj': lambda: (2, \"nsubj=agency\"), \n",
    "             'advmod': lambda: (4, \"extract_for('plain', 'advmod')\"),\n",
    "             'cop': lambda: (2, \"cop='c_aux'\"),\n",
    "             'head_amod': lambda: (2, \"head_amod=interr('what')\"),\n",
    "             '時間': lambda: (4, \"extract_for('plain+date_search+date_parse', '時間')\"),\n",
    "            }\n",
    "    if chunk.name in gen_map:\n",
    "        pats.append(gen_map[chunk.name]())\n",
    "\n",
    "def induce_pattern(pat, ds):\n",
    "    if enable_verbose:\n",
    "        tc.emp('magenta', pat)    \n",
    "    def gen_verb(ind='verb', prefix='behave'):\n",
    "        spec=[d for d in ds if d['indicator']==f'[{ind}]']\n",
    "        if spec:\n",
    "            ref=spec[0]['spec']\n",
    "        else:\n",
    "            ref=pat.translate.replace(' ', '_')\n",
    "        return f\"pat(5, name='{prefix}_{ref}').verb\"\n",
    "    def gen_root():\n",
    "        spec=[d for d in ds if d['indicator']=='[root]']\n",
    "        if spec:\n",
    "            ref=spec[0]['spec']\n",
    "        else:\n",
    "            ref=pat.translate.replace(' ', '_')\n",
    "        return f\"pat(5, name='ana_{ref}').root\"\n",
    "    def gen_cop():\n",
    "        spec=[d for d in ds if d['indicator']=='head']\n",
    "        if spec:\n",
    "            ref=spec[0]['spec']\n",
    "        else:\n",
    "            ref=pat.head_trans.replace(' ', '_') if pat.lang!='en' else pat.head.replace(' ', '_')\n",
    "        return f\"pat(5, name='desc_{ref}').cop\"    \n",
    "    \n",
    "    domap={'verb_domains': gen_verb,\n",
    "           'aux_domains': gen_cop,\n",
    "           'subj_domains': gen_cop,\n",
    "           'root_domains': gen_root,\n",
    "           'predicate': lambda: gen_verb('predicate', 'predict'),\n",
    "          }\n",
    "    return domap[pat.type]()\n",
    "\n",
    "######\n",
    "sinkers = Sinkers()\n",
    "for serial, r in enumerate(rs):\n",
    "    type_name = r['type']\n",
    "    theme=type_name.split('_')[0]\n",
    "    print(type_name)\n",
    "    meta = build_meta(r, data)\n",
    "    # print(f\"meta keys {meta.keys()}\")\n",
    "    \n",
    "    mod_rs=langspecs.check_langspec(data['lang'], meta, r['domains'], type_name)\n",
    "    sinkers.add_module_results(mod_rs)\n",
    "    \n",
    "    # infers\n",
    "    pats=[]\n",
    "    def do_infers(ds):\n",
    "        df = sagas.to_df(r['domains'], ['rel', 'index', 'text', 'lemma', 'children', 'features'])\n",
    "\n",
    "        pat=proc_word(type_name, r['word'], \n",
    "                      r['head'] if 'head' in r else '', \n",
    "                      r['index'],\n",
    "                      data['lang'])\n",
    "        pat_r=induce_pattern(to_obj(pat), ds)\n",
    "        parts=proc_children_column(df, data['lang'])    \n",
    "        for part in parts:\n",
    "            induce_part(to_obj(part), pats, type_name)\n",
    "        # display_synsets(f\"[{theme}]\", meta, r, data['lang'])    \n",
    "        return pat_r\n",
    "    \n",
    "    # induce with wordnet\n",
    "    ds=display_synsets(f\"[{theme}]\", meta, r, data['lang'], collect=True)    \n",
    "    pat_r=do_infers(ds)\n",
    "    indicators=[]\n",
    "    for el in ds:\n",
    "        if el['indicator'] not in indicators:\n",
    "            induce_spec(to_obj(el), pats, type_name)\n",
    "            indicators.append(el['indicator'])\n",
    "    \n",
    "    pats=sorted(pats, key=lambda pat: -pat[0])\n",
    "    paras=', '.join(p[1] for p in pats)\n",
    "    print(f\"{pat_r}({paras}),\")\n",
    "    \n",
    "    # debug\n",
    "    print('*', '-'*45)\n",
    "    pprint(ds)\n",
    "\n",
    "sinkers.process_with_sinkers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-03T03:56:41.942546Z",
     "start_time": "2020-03-03T03:56:41.930451Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'false'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pickup=''\n",
    "'true' if pickup else 'false'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
