{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T14:41:47.823811Z",
     "start_time": "2020-03-24T14:41:43.796266Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-24 22:41:43 INFO: Loading these models for language: en (English):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | ewt       |\n",
      "| pos       | ewt       |\n",
      "| lemma     | ewt       |\n",
      "| depparse  | ewt       |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "2020-03-24 22:41:43 INFO: Use device: cpu\n",
      "2020-03-24 22:41:43 INFO: Loading: tokenize\n",
      "2020-03-24 22:41:43 INFO: Loading: pos\n",
      "2020-03-24 22:41:45 INFO: Loading: lemma\n",
      "2020-03-24 22:41:45 INFO: Loading: depparse\n",
      "2020-03-24 22:41:46 INFO: Loading: ner\n",
      "2020-03-24 22:41:47 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "pipeline = stanza.Pipeline(dir='/pi/ai/corenlp/1.0', package='default', lang='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T14:45:44.173519Z",
     "start_time": "2020-03-24T14:45:43.987954Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Token id=1;words=[<Word id=1;text=i;lemma=i;upos=PRON;xpos=PRP;feats=Case=Nom|Number=Sing|Person=1|PronType=Prs;head=4;deprel=nsubj>]>\n",
      "<Token id=2;words=[<Word id=2;text=am;lemma=be;upos=AUX;xpos=VBP;feats=Mood=Ind|Number=Sing|Person=1|Tense=Pres|VerbForm=Fin;head=4;deprel=cop>]>\n",
      "<Token id=3;words=[<Word id=3;text=a;lemma=a;upos=DET;xpos=DT;feats=Definite=Ind|PronType=Art;head=4;deprel=det>]>\n",
      "<Token id=4;words=[<Word id=4;text=student;lemma=student;upos=NOUN;xpos=NN;feats=Number=Sing;head=0;deprel=root>]>\n"
     ]
    }
   ],
   "source": [
    "doc = pipeline('i am a student')\n",
    "doc.sentences[0].print_tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T14:45:52.440785Z",
     "start_time": "2020-03-24T14:45:52.427103Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('i', '4', 'nsubj')\n",
      "('am', '4', 'cop')\n",
      "('a', '4', 'det')\n",
      "('student', '0', 'root')\n"
     ]
    }
   ],
   "source": [
    "doc.sentences[0].print_dependencies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T14:57:21.130209Z",
     "start_time": "2020-03-24T14:57:14.112070Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-24 22:57:14 INFO: Loading these models for language: zh-hans (Simplified_Chinese):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | gsdsimp   |\n",
      "| pos       | gsdsimp   |\n",
      "| lemma     | gsdsimp   |\n",
      "| depparse  | gsdsimp   |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "2020-03-24 22:57:14 INFO: Use device: cpu\n",
      "2020-03-24 22:57:14 INFO: Loading: tokenize\n",
      "2020-03-24 22:57:14 INFO: Loading: pos\n",
      "2020-03-24 22:57:17 INFO: Loading: lemma\n",
      "2020-03-24 22:57:17 INFO: Loading: depparse\n",
      "2020-03-24 22:57:20 INFO: Loading: ner\n",
      "2020-03-24 22:57:21 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "# pipeline_sr = stanza.Pipeline(dir='/pi/ai/corenlp/1.0', package='default', lang='sr')\n",
    "pipeline_zh = stanza.Pipeline(dir='/pi/ai/corenlp/1.0', package='default', lang='zh-hans')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T14:57:39.947260Z",
     "start_time": "2020-03-24T14:57:39.770440Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Token id=1;words=[<Word id=1;text=我;lemma=我;upos=PRON;xpos=PRP;feats=Person=1;head=4;deprel=nsubj>]>\n",
      "<Token id=2;words=[<Word id=2;text=明天;lemma=明天;upos=NOUN;xpos=NN;head=4;deprel=nmod:tmod>]>\n",
      "<Token id=3;words=[<Word id=3;text=去;lemma=去;upos=ADV;xpos=RB;head=4;deprel=mark>]>\n",
      "<Token id=4;words=[<Word id=4;text=上班;lemma=上班;upos=VERB;xpos=VV;head=0;deprel=root>]>\n",
      "('我', '4', 'nsubj')\n",
      "('明天', '4', 'nmod:tmod')\n",
      "('去', '4', 'mark')\n",
      "('上班', '0', 'root')\n"
     ]
    }
   ],
   "source": [
    "# doc = pipeline_sr('Када вози аутобус у центар града?')\n",
    "doc = pipeline_zh('我明天去上班')\n",
    "doc.sentences[0].print_tokens()\n",
    "doc.sentences[0].print_dependencies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T07:31:39.637292Z",
     "start_time": "2020-03-25T07:31:39.220715Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Token id=1;words=[<Word id=1;text=我;lemma=我;upos=PRON;xpos=PRP;feats=Person=1;head=6;deprel=nsubj>]>\n",
      "<Token id=2;words=[<Word id=2;text=明天;lemma=明天;upos=NOUN;xpos=NN;head=6;deprel=nmod:tmod>]>\n",
      "<Token id=3;words=[<Word id=3;text=去;lemma=去;upos=VERB;xpos=VV;head=6;deprel=advcl>]>\n",
      "<Token id=4;words=[<Word id=4;text=体育;lemma=体育;upos=NOUN;xpos=NN;head=5;deprel=nmod>]>\n",
      "<Token id=5;words=[<Word id=5;text=中心;lemma=中心;upos=NOUN;xpos=NN;head=3;deprel=obj>]>\n",
      "<Token id=6;words=[<Word id=6;text=上班;lemma=上班;upos=VERB;xpos=VV;head=0;deprel=root>]>\n",
      "<class 'stanza.models.common.doc.Word'> 上班 nsubj 我\n",
      "<class 'stanza.models.common.doc.Word'> 上班 nmod:tmod 明天\n",
      "<class 'stanza.models.common.doc.Word'> 上班 advcl 去\n",
      "<class 'stanza.models.common.doc.Word'> 中心 nmod 体育\n",
      "<class 'stanza.models.common.doc.Word'> 去 obj 中心\n",
      "<class 'stanza.models.common.doc.Word'> ROOT root 上班\n"
     ]
    }
   ],
   "source": [
    "from sagas.nlu.stanza_helper import get_nlp\n",
    "nlp=get_nlp('zh')\n",
    "sts=nlp('我明天去体育中心上班')\n",
    "st=sts.sentences[0]\n",
    "st.print_tokens()\n",
    "for dep in st.dependencies:\n",
    "    print(type(dep[0]), dep[0].text, dep[1], dep[2].text)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T12:34:54.854839Z",
     "start_time": "2020-03-25T12:34:54.505711Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Token id=1;words=[<Word id=1;text=Por;lemma=por;upos=ADP;head=2;deprel=case>]>\n",
      "<Token id=2;words=[<Word id=2;text=que;lemma=que;upos=PRON;feats=Gender=Masc|Number=Sing|PronType=Rel;head=5;deprel=obj>]>\n",
      "<Token id=3;words=[<Word id=3;text=você;lemma=você;upos=PRON;feats=Case=Nom|Gender=Unsp|Number=Sing|Person=3|PronType=Prs;head=5;deprel=nsubj>]>\n",
      "<Token id=4;words=[<Word id=4;text=não;lemma=não;upos=ADV;feats=Polarity=Neg;head=5;deprel=advmod>]>\n",
      "<Token id=5;words=[<Word id=5;text=perguntou;lemma=perguntar;upos=VERB;feats=Mood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin;head=0;deprel=root>]>\n",
      "<Token id=6;words=[<Word id=6;text=?;lemma=?;upos=PUNCT;head=5;deprel=punct>]>\n",
      "<class 'stanza.models.common.doc.Word'> que case Por\n",
      "<class 'stanza.models.common.doc.Word'> perguntou obj que\n",
      "<class 'stanza.models.common.doc.Word'> perguntou nsubj você\n",
      "<class 'stanza.models.common.doc.Word'> perguntou advmod não\n",
      "<class 'stanza.models.common.doc.Word'> ROOT root perguntou\n",
      "<class 'stanza.models.common.doc.Word'> perguntou punct ?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('1', 'Por', 'por', 'case', None, 2, 'Por', 'ADP', None, None),\n",
       " ('2', 'que', 'que', 'obj', None, 5, 'que', 'PRON', None, None),\n",
       " ('3', 'você', 'você', 'nsubj', None, 5, 'você', 'PRON', None, None),\n",
       " ('4', 'não', 'não', 'advmod', None, 5, 'não', 'ADV', None, None),\n",
       " ('5',\n",
       "  'perguntou',\n",
       "  'perguntar',\n",
       "  'root',\n",
       "  None,\n",
       "  0,\n",
       "  'perguntou',\n",
       "  'VERB',\n",
       "  None,\n",
       "  None),\n",
       " ('6', '?', '?', 'punct', None, 5, '?', 'PUNCT', None, None)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagas.nlu.stanza_helper import get_nlp\n",
    "nlp=get_nlp('pt')\n",
    "sts=nlp('Por que você não perguntou?')\n",
    "st=sts.sentences[0]\n",
    "st.print_tokens()\n",
    "for dep in st.dependencies:\n",
    "    print(type(dep[0]), dep[0].text, dep[1], dep[2].text)    \n",
    "[(w.id, w.text, w.lemma, w.deprel, w.deps, \n",
    "  w.head, w.parent.text, w.upos, w.xpos, \n",
    "  w.parent.ner) for w in st.words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T07:53:50.608119Z",
     "start_time": "2020-03-25T07:53:50.599423Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', '我', '我', 'nsubj', None, 6, '我', 'O'),\n",
       " ('2', '明天', '明天', 'nmod:tmod', None, 6, '明天', 'S-DATE'),\n",
       " ('3', '去', '去', 'advcl', None, 6, '去', 'O'),\n",
       " ('4', '体育', '体育', 'nmod', None, 5, '体育', 'O'),\n",
       " ('5', '中心', '中心', 'obj', None, 3, '中心', 'O'),\n",
       " ('6', '上班', '上班', 'root', None, 0, '上班', 'O')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(w.id, w.text, w.lemma, w.deprel, w.deps, w.head, w.parent.text, w.parent.ner) for w in st.words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T04:34:48.952250Z",
     "start_time": "2020-03-25T04:34:48.803519Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total tokens and words: 9 9\n",
      "<Token id=1;words=[<Word id=1;text=僕;lemma=僕;upos=PRON;xpos=NP;head=5;deprel=nsubj>]>\n",
      "<Token id=2;words=[<Word id=2;text=は;lemma=は;upos=ADP;xpos=PK;head=1;deprel=case>]>\n",
      "<Token id=3;words=[<Word id=3;text=君;lemma=君;upos=PRON;xpos=NP;head=5;deprel=obj>]>\n",
      "<Token id=4;words=[<Word id=4;text=を;lemma=を;upos=ADP;xpos=PS;head=3;deprel=case>]>\n",
      "<Token id=5;words=[<Word id=5;text=探し;lemma=探す;upos=VERB;xpos=VV;head=0;deprel=root>]>\n",
      "<Token id=6;words=[<Word id=6;text=始め;lemma=始める;upos=AUX;xpos=AV;head=5;deprel=aux>]>\n",
      "<Token id=7;words=[<Word id=7;text=た;lemma=た;upos=AUX;xpos=AV;head=5;deprel=aux>]>\n",
      "<Token id=8;words=[<Word id=8;text=よ;lemma=よ;upos=PART;xpos=PE;head=5;deprel=mark>]>\n",
      "<Token id=9;words=[<Word id=9;text=。;lemma=。;upos=PUNCT;xpos=SYM;head=5;deprel=punct>]>\n"
     ]
    }
   ],
   "source": [
    "nlp=get_nlp('ja')\n",
    "sts=nlp('僕は君を探し始めたよ。')\n",
    "print('total tokens and words:', sts.num_tokens, sts.num_words)\n",
    "sts.sentences[0].print_tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-27T04:53:23.625454Z",
     "start_time": "2020-03-27T04:53:23.520419Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total tokens and words: 4 4\n",
      "<Token id=1;words=[<Word id=1;text=Rezervasyonumu>]>\n",
      "<Token id=2;words=[<Word id=2;text=onaylamak>]>\n",
      "<Token id=3;words=[<Word id=3;text=istiyorum>]>\n",
      "<Token id=4;words=[<Word id=4;text=.>]>\n"
     ]
    }
   ],
   "source": [
    "from sagas.nlu.stanza_helper import get_nlp\n",
    "nlp=get_nlp('tr', processors='tokenize,mwt')\n",
    "# nlp=get_nlp('tr', processors='tokenize,lemma,pos')\n",
    "# sts=nlp('Bir bira isterim.')\n",
    "sts=nlp('Rezervasyonumu onaylamak istiyorum.')\n",
    "print('total tokens and words:', sts.num_tokens, sts.num_words)\n",
    "sts.sentences[0].print_tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-27T04:45:07.007066Z",
     "start_time": "2020-03-27T04:45:06.868609Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total tokens and words: 24 26\n",
      "token: facebook \t\twords: [<Word id=1;text=facebook;upos=NOUN;xpos=subst:sg:nom:f;feats=Case=Nom|Gender=Fem|Number=Sing>]\n",
      "token: poczta   \t\twords: [<Word id=1;text=poczta;upos=NOUN;xpos=subst:sg:nom:f;feats=Case=Nom|Gender=Fem|Number=Sing>]\n",
      "token: onet     \t\twords: [<Word id=2;text=onet;upos=NOUN;xpos=subst:sg:nom:f;feats=Case=Nom|Gender=Fem|Number=Sing>]\n",
      "token: nowoczesna\t\twords: [<Word id=1;text=nowoczesna;upos=ADJ;xpos=adj:sg:nom:f:pos;feats=Case=Nom|Degree=Pos|Gender=Fem|Number=Sing>]\n",
      "token: diagnostyka\t\twords: [<Word id=2;text=diagnostyka;upos=NOUN;xpos=subst:sg:nom:f;feats=Case=Nom|Gender=Fem|Number=Sing>]\n",
      "token: raka     \t\twords: [<Word id=3;text=raka;upos=NOUN;xpos=subst:sg:gen:m2;feats=Animacy=Nhum|Case=Gen|Gender=Masc|Number=Sing>]\n",
      "token: prostaty \t\twords: [<Word id=4;text=prostaty;upos=ADJ;xpos=adj:sg:gen:m3:pos;feats=Animacy=Inan|Case=Gen|Degree=Pos|Gender=Masc|Number=Sing>]\n",
      "token: pkp      \t\twords: [<Word id=1;text=pkp;upos=NOUN;xpos=subst:sg:nom:n:ncol;feats=Case=Nom|Gender=Neut|Number=Sing>]\n",
      "token: rozkład  \t\twords: [<Word id=2;text=rozkład;upos=NOUN;xpos=subst:sg:nom:m3;feats=Animacy=Inan|Case=Nom|Gender=Masc|Number=Sing>]\n",
      "token: jazdy    \t\twords: [<Word id=3;text=jazdy;upos=NOUN;xpos=subst:sg:gen:f;feats=Case=Gen|Gender=Fem|Number=Sing>]\n",
      "token: boli     \t\twords: [<Word id=1;text=boli;upos=VERB;xpos=fin:sg:ter:imperf;feats=Aspect=Imp|Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin|Voice=Act>]\n",
      "token: mnie     \t\twords: [<Word id=2;text=mnie;upos=PRON;xpos=ppron12:sg:acc:m1:pri:akc;feats=Animacy=Hum|Case=Acc|Gender=Masc|Number=Sing|Person=1|PronType=Prs|Variant=Long>]\n",
      "token: prawe    \t\twords: [<Word id=3;text=prawe;upos=ADJ;xpos=adj:sg:nom:n:pos;feats=Case=Nom|Degree=Pos|Gender=Neut|Number=Sing>]\n",
      "token: jądro    \t\twords: [<Word id=4;text=jądro;upos=NOUN;xpos=subst:sg:nom:n:ncol;feats=Case=Nom|Gender=Neut|Number=Sing>]\n",
      "token: i        \t\twords: [<Word id=5;text=i;upos=CCONJ;xpos=conj>]\n",
      "token: podbrzusze\t\twords: [<Word id=6;text=podbrzusze;upos=NOUN;xpos=subst:sg:nom:n:ncol;feats=Case=Nom|Gender=Neut|Number=Sing>]\n",
      "word: facebook \t\ttoken parent:1-facebook\n",
      "word: poczta   \t\ttoken parent:1-poczta\n",
      "word: onet     \t\ttoken parent:2-onet\n",
      "word: nowoczesna\t\ttoken parent:1-nowoczesna\n",
      "word: diagnostyka\t\ttoken parent:2-diagnostyka\n",
      "word: raka     \t\ttoken parent:3-raka\n",
      "word: prostaty \t\ttoken parent:4-prostaty\n",
      "word: pkp      \t\ttoken parent:1-pkp\n",
      "word: rozkład  \t\ttoken parent:2-rozkład\n",
      "word: jazdy    \t\ttoken parent:3-jazdy\n",
      "word: boli     \t\ttoken parent:1-boli\n",
      "word: mnie     \t\ttoken parent:2-mnie\n",
      "word: prawe    \t\ttoken parent:3-prawe\n",
      "word: jądro    \t\ttoken parent:4-jądro\n",
      "word: i        \t\ttoken parent:5-i\n",
      "word: podbrzusze\t\ttoken parent:6-podbrzusze\n"
     ]
    }
   ],
   "source": [
    "from sagas.nlu.stanza_helper import get_nlp\n",
    "nlp=get_nlp('fr', processors='tokenize,mwt')\n",
    "sts=nlp(\"Alors encore inconnu du grand public, Emmanuel Macron devient en 2014 ministre de l'Économie, de l'Industrie et du Numérique.\")\n",
    "print('total tokens and words:', sts.num_tokens, sts.num_words)\n",
    "# sts.sentences[0].print_tokens()\n",
    "token_to_words = \"\\n\".join(\n",
    "        [f'token: {token.text.ljust(9)}\\t\\twords: [{\", \".join([word.pretty_print() for word in token.words])}]' for sent in doc.sentences for token in sent.tokens]\n",
    "    ).strip()\n",
    "word_to_token = \"\\n\".join(\n",
    "    [f'word: {word.text.ljust(9)}\\t\\ttoken parent:{word.parent.id+\"-\"+word.parent.text}'\n",
    "     for sent in doc.sentences for word in sent.words]).strip()\n",
    "print(token_to_words)\n",
    "print(word_to_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T04:57:19.840574Z",
     "start_time": "2020-03-25T04:57:16.862439Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-25 12:57:16 INFO: Loading these models for language: id (Indonesian):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | gsd     |\n",
      "| pos       | gsd     |\n",
      "| lemma     | gsd     |\n",
      "| depparse  | gsd     |\n",
      "=======================\n",
      "\n",
      "2020-03-25 12:57:16 INFO: Use device: cpu\n",
      "2020-03-25 12:57:16 INFO: Loading: tokenize\n",
      "2020-03-25 12:57:16 INFO: Loading: pos\n",
      "2020-03-25 12:57:18 INFO: Loading: lemma\n",
      "2020-03-25 12:57:18 INFO: Loading: depparse\n",
      "2020-03-25 12:57:19 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total tokens and words: 4 4\n",
      "<Token id=1;words=[<Word id=1;text=Bola;lemma=bola;upos=NOUN;xpos=NSD;feats=Number=Sing;head=0;deprel=root>]>\n",
      "<Token id=2;words=[<Word id=2;text=Dimas;lemma=dimas;upos=PROPN;xpos=VSP;feats=Number=Sing|Voice=Pass;head=1;deprel=flat>]>\n",
      "<Token id=3;words=[<Word id=3;text=putih;lemma=putih;upos=ADJ;xpos=ASP;feats=Degree=Pos|Number=Sing;head=2;deprel=amod>]>\n",
      "<Token id=4;words=[<Word id=4;text=.;lemma=.;upos=PUNCT;xpos=Z--;head=1;deprel=punct>]>\n"
     ]
    }
   ],
   "source": [
    "sents='Bola Dimas putih.'\n",
    "nlp=get_nlp('id')\n",
    "sts=nlp(sents)\n",
    "print('total tokens and words:', sts.num_tokens, sts.num_words)\n",
    "sts.sentences[0].print_tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-26T02:56:28.755043Z",
     "start_time": "2020-03-26T02:56:28.541056Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total tokens and words: 8 8\n",
      "<Token id=1;words=[<Word id=1;text=Tôi;lemma=Tôi;upos=PROPN;xpos=P;head=2;deprel=nsubj>]>\n",
      "<Token id=2;words=[<Word id=2;text=muốn;lemma=muốn;upos=VERB;xpos=V;head=0;deprel=root>]>\n",
      "<Token id=3;words=[<Word id=3;text=đăng ký;lemma=đăng ký;upos=VERB;xpos=V;head=2;deprel=xcomp>]>\n",
      "<Token id=4;words=[<Word id=4;text=một chuyến;lemma=một chuyến;upos=NOUN;xpos=N;head=3;deprel=obj>]>\n",
      "<Token id=5;words=[<Word id=5;text=bay;lemma=bay;upos=VERB;xpos=V;head=4;deprel=xcomp>]>\n",
      "<Token id=6;words=[<Word id=6;text=sang;lemma=sang;upos=VERB;xpos=V;head=5;deprel=xcomp>]>\n",
      "<Token id=7;words=[<Word id=7;text=Athen;lemma=Athen;upos=NOUN;xpos=Np;head=6;deprel=obj>]>\n",
      "<Token id=8;words=[<Word id=8;text=.;lemma=.;upos=PUNCT;xpos=.;head=2;deprel=punct>]>\n"
     ]
    }
   ],
   "source": [
    "sents='Tôi muốn đăng ký một chuyến bay sang Athen.'\n",
    "nlp=get_nlp('vi')\n",
    "sts=nlp(sents)\n",
    "print('total tokens and words:', sts.num_tokens, sts.num_words)\n",
    "sts.sentences[0].print_tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T04:39:26.603328Z",
     "start_time": "2020-03-25T04:39:26.394166Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total tokens and words: 4 4\n",
      "<Token id=1;words=[<Word id=1;text=창가;lemma=창가;upos=NOUN;xpos=ncn+jcs;head=3;deprel=nsubj>]>\n",
      "<Token id=2;words=[<Word id=2;text=자리를;lemma=자리+를;upos=NOUN;xpos=ncn+jco;head=3;deprel=obj>]>\n",
      "<Token id=3;words=[<Word id=3;text=주세요,;lemma=주세요+,;upos=CCONJ;xpos=pvg+ecc;head=0;deprel=root>]>\n",
      "<Token id=4;words=[<Word id=4;text=비흡연석으로요.;lemma=비흡연석으로요.;upos=ADV;xpos=ncn+ncn+jca+jxc;head=3;deprel=conj>]>\n"
     ]
    }
   ],
   "source": [
    "nlp=get_nlp('ko')\n",
    "# sts=nlp('그게 당신의 여행가방이에요?') # geuge dangsin-ui yeohaeng-gabang-ieyo?\n",
    "sts=nlp('창가 자리를 주세요, 비흡연석으로요.')\n",
    "print('total tokens and words:', sts.num_tokens, sts.num_words)\n",
    "sts.sentences[0].print_tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T04:44:59.993607Z",
     "start_time": "2020-03-25T04:44:59.654829Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total tokens and words: 8 8\n",
      "<Token id=1;words=[<Word id=1;text=‫میں;lemma=‫میں;upos=PRON;xpos=PRP;feats=Case=Acc,Dat|Number=Sing|Person=3|PronType=Prs;head=6;deprel=nsubj>]>\n",
      "<Token id=2;words=[<Word id=2;text=ایتھنز;lemma=ایتھنز;upos=PROPN;xpos=NNP;feats=Case=Acc|Gender=Masc|Number=Sing|Person=3;head=4;deprel=nmod>]>\n",
      "<Token id=3;words=[<Word id=3;text=کی;lemma=کا;upos=ADP;xpos=PSP;feats=AdpType=Post|Case=Nom|Gender=Fem|Number=Sing;head=2;deprel=case>]>\n",
      "<Token id=4;words=[<Word id=4;text=فلائٹ;lemma=فلائٹ;upos=NOUN;xpos=NN;feats=Case=Nom|Gender=Masc|Number=Sing|Person=3;head=6;deprel=obj>]>\n",
      "<Token id=5;words=[<Word id=5;text=بک;lemma=بک;upos=NOUN;xpos=NN;feats=Case=Nom|Gender=Masc|Number=Sing|Person=3;head=6;deprel=compound>]>\n",
      "<Token id=6;words=[<Word id=6;text=کرنا;lemma=کر;upos=VERB;xpos=VM;feats=Gender=Masc|Number=Sing|VerbForm=Inf|Voice=Act;head=0;deprel=root>]>\n",
      "<Token id=7;words=[<Word id=7;text=چاہتا;lemma=چاہ;upos=AUX;xpos=VAUX;feats=Aspect=Imp|Gender=Masc|Number=Sing|VerbForm=Part;head=6;deprel=aux>]>\n",
      "<Token id=8;words=[<Word id=8;text=ہوں‬;lemma=ہوں‬;upos=PUNCT;xpos=SYM;head=6;deprel=punct>]>\n",
      "('\\u202bمیں', '6', 'nsubj')\n",
      "('ایتھنز', '4', 'nmod')\n",
      "('کی', '2', 'case')\n",
      "('فلائٹ', '6', 'obj')\n",
      "('بک', '6', 'compound')\n",
      "('کرنا', '0', 'root')\n",
      "('چاہتا', '6', 'aux')\n",
      "('ہوں\\u202c', '6', 'punct')\n"
     ]
    }
   ],
   "source": [
    "nlp=get_nlp('ur')\n",
    "sts=nlp('‫میں ایتھنز کی فلائٹ بک کرنا چاہتا ہوں‬')\n",
    "print('total tokens and words:', sts.num_tokens, sts.num_words)\n",
    "sts.sentences[0].print_tokens()\n",
    "sts.sentences[0].print_dependencies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-26T07:19:37.281047Z",
     "start_time": "2020-03-26T07:19:37.268718Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Pipeline'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(nlp.__class__.__name__)\n",
    "type(nlp).__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-27T04:39:49.803278Z",
     "start_time": "2020-03-27T04:39:48.070955Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-27 12:39:48 INFO: Loading these models for language: pl (Polish):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | pdb     |\n",
      "| pos       | pdb     |\n",
      "=======================\n",
      "\n",
      "2020-03-27 12:39:48 INFO: Use device: cpu\n",
      "2020-03-27 12:39:48 INFO: Loading: tokenize\n",
      "2020-03-27 12:39:48 INFO: Loading: pos\n",
      "2020-03-27 12:39:49 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "facebook NOUN\n",
      "poczta NOUN\n",
      "onet NOUN\n",
      "nowoczesna ADJ\n",
      "diagnostyka NOUN\n",
      "raka NOUN\n",
      "prostaty ADJ\n",
      "pkp NOUN\n",
      "rozkład NOUN\n",
      "jazdy NOUN\n",
      "boli VERB\n",
      "mnie PRON\n",
      "prawe ADJ\n",
      "jądro NOUN\n",
      "i CCONJ\n",
      "podbrzusze NOUN\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/stanfordnlp/stanza/issues/220\n",
    "import stanza\n",
    "# stanza.download('pl', package='pdb')\n",
    "\n",
    "nlp = stanza.Pipeline(lang='pl', processors='tokenize,pos', package='pdb')\n",
    "\n",
    "queries = [\n",
    "   'facebook',\n",
    "   'poczta onet',\n",
    "   'nowoczesna diagnostyka raka prostaty',\n",
    "   'pkp rozkład jazdy',\n",
    "   'boli mnie prawe jądro i podbrzusze'\n",
    "]\n",
    "\n",
    "batch = \"\\n\\n\".join(queries)\n",
    "doc = nlp(batch)\n",
    "\n",
    "for sent in doc.sentences:\n",
    "    for word in sent.words:\n",
    "        print(word.text, word.upos)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
