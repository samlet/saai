{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T08:22:01.218745Z",
     "start_time": "2020-06-23T08:22:00.631155Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from spacy.vocab import Vocab\n",
    "import spacy\n",
    "from spacy.kb import KnowledgeBase\n",
    "\n",
    "\n",
    "# Q2146908 (Russ Cochran): American golfer\n",
    "# Q7381115 (Russ Cochran): publisher\n",
    "ENTITIES = {\"Q2146908\": (\"American golfer\", 342), \"Q7381115\": (\"publisher\", 17)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T08:25:40.354992Z",
     "start_time": "2020-06-23T08:25:28.089683Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model 'en_core_web_md'\n",
      "\n",
      "2 kb entities: ['Q2146908', 'Q7381115']\n",
      "1 kb aliases: ['Russ Cochran']\n",
      "\n",
      "Saved KB to kb.out/kb\n",
      "Saved vocab to kb.out/vocab\n",
      "\n",
      "Loading vocab from kb.out/vocab\n",
      "Loading KB from kb.out/kb\n",
      "\n",
      "2 kb entities: ['Q2146908', 'Q7381115']\n",
      "1 kb aliases: ['Russ Cochran']\n"
     ]
    }
   ],
   "source": [
    "def create_kb(model=None, output_dir=None):\n",
    "    \"\"\"Load the model and create the KB with pre-defined entity encodings.\n",
    "    If an output_dir is provided, the KB will be stored there in a file 'kb'.\n",
    "    The updated vocab will also be written to a directory in the output_dir.\"\"\"\n",
    "\n",
    "    nlp = spacy.load(model)  # load existing spaCy model\n",
    "    print(\"Loaded model '%s'\" % model)\n",
    "\n",
    "    # check the length of the nlp vectors\n",
    "    if \"vectors\" not in nlp.meta or not nlp.vocab.vectors.size:\n",
    "        raise ValueError(\n",
    "            \"The `nlp` object should have access to pretrained word vectors, \"\n",
    "            \" cf. https://spacy.io/usage/models#languages.\"\n",
    "        )\n",
    "\n",
    "    # You can change the dimension of vectors in your KB by using an encoder that changes the dimensionality.\n",
    "    # For simplicity, we'll just use the original vector dimension here instead.\n",
    "    vectors_dim = nlp.vocab.vectors.shape[1]\n",
    "    kb = KnowledgeBase(vocab=nlp.vocab, entity_vector_length=vectors_dim)\n",
    "\n",
    "    # set up the data\n",
    "    entity_ids = []\n",
    "    descr_embeddings = []\n",
    "    freqs = []\n",
    "    for key, value in ENTITIES.items():\n",
    "        desc, freq = value\n",
    "        entity_ids.append(key)\n",
    "        descr_embeddings.append(nlp(desc).vector)\n",
    "        freqs.append(freq)\n",
    "\n",
    "    # set the entities, can also be done by calling `kb.add_entity` for each entity\n",
    "    kb.set_entities(entity_list=entity_ids, freq_list=freqs, vector_list=descr_embeddings)\n",
    "\n",
    "    # adding aliases, the entities need to be defined in the KB beforehand\n",
    "    kb.add_alias(\n",
    "        alias=\"Russ Cochran\",\n",
    "        entities=[\"Q2146908\", \"Q7381115\"],\n",
    "        probabilities=[0.24, 0.7],  # the sum of these probabilities should not exceed 1\n",
    "    )\n",
    "\n",
    "    # test the trained model\n",
    "    print()\n",
    "    _print_kb(kb)\n",
    "\n",
    "    # save model to output directory\n",
    "    if output_dir is not None:\n",
    "        output_dir = Path(output_dir)\n",
    "        if not output_dir.exists():\n",
    "            output_dir.mkdir()\n",
    "        kb_path = str(output_dir / \"kb\")\n",
    "        kb.dump(kb_path)\n",
    "        print()\n",
    "        print(\"Saved KB to\", kb_path)\n",
    "\n",
    "        vocab_path = output_dir / \"vocab\"\n",
    "        kb.vocab.to_disk(vocab_path)\n",
    "        print(\"Saved vocab to\", vocab_path)\n",
    "\n",
    "        print()\n",
    "\n",
    "        # test the saved model\n",
    "        # always reload a knowledge base with the same vocab instance!\n",
    "        print(\"Loading vocab from\", vocab_path)\n",
    "        print(\"Loading KB from\", kb_path)\n",
    "        vocab2 = Vocab().from_disk(vocab_path)\n",
    "        kb2 = KnowledgeBase(vocab=vocab2)\n",
    "        kb2.load_bulk(kb_path)\n",
    "        print()\n",
    "        _print_kb(kb2)\n",
    "\n",
    "def _print_kb(kb):\n",
    "    print(kb.get_size_entities(), \"kb entities:\", kb.get_entity_strings())\n",
    "    print(kb.get_size_aliases(), \"kb aliases:\", kb.get_alias_strings())\n",
    "\n",
    "create_kb('en_core_web_md', 'kb.out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T08:26:12.898904Z",
     "start_time": "2020-06-23T08:26:12.890154Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "from spacy.vocab import Vocab\n",
    "\n",
    "import spacy\n",
    "from spacy.kb import KnowledgeBase\n",
    "from spacy.pipeline import EntityRuler\n",
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "\n",
    "def sample_train_data():\n",
    "    train_data = []\n",
    "\n",
    "    # Q2146908 (Russ Cochran): American golfer\n",
    "    # Q7381115 (Russ Cochran): publisher\n",
    "\n",
    "    text_1 = \"Russ Cochran his reprints include EC Comics.\"\n",
    "    dict_1 = {(0, 12): {\"Q7381115\": 1.0, \"Q2146908\": 0.0}}\n",
    "    train_data.append((text_1, {\"links\": dict_1}))\n",
    "\n",
    "    text_2 = \"Russ Cochran has been publishing comic art.\"\n",
    "    dict_2 = {(0, 12): {\"Q7381115\": 1.0, \"Q2146908\": 0.0}}\n",
    "    train_data.append((text_2, {\"links\": dict_2}))\n",
    "\n",
    "    text_3 = \"Russ Cochran captured his first major title with his son as caddie.\"\n",
    "    dict_3 = {(0, 12): {\"Q7381115\": 0.0, \"Q2146908\": 1.0}}\n",
    "    train_data.append((text_3, {\"links\": dict_3}))\n",
    "\n",
    "    text_4 = \"Russ Cochran was a member of University of Kentucky's golf team.\"\n",
    "    dict_4 = {(0, 12): {\"Q7381115\": 0.0, \"Q2146908\": 1.0}}\n",
    "    train_data.append((text_4, {\"links\": dict_4}))\n",
    "\n",
    "    return train_data\n",
    "\n",
    "\n",
    "# training data\n",
    "TRAIN_DATA = sample_train_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T08:35:44.841719Z",
     "start_time": "2020-06-23T08:35:40.541991Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created blank 'en' model with vocab from './kb.out/vocab'\n",
      "Loaded Knowledge Base from './kb.out/kb'\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[E188] Could not match the gold entity links to entities in the doc - make sure the gold EL data refers to valid results of the named entity recognizer in the `nlp` pipeline.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-e9531204e16c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tokens\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ment_type_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ment_kb_id_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m \u001b[0mtrain_nel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./kb.out/kb'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'./kb.out/vocab'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'./nel.out'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-e9531204e16c>\u001b[0m in \u001b[0;36mtrain_nel\u001b[0;34m(kb_path, vocab_path, output_dir, n_iter)\u001b[0m\n\u001b[1;32m     67\u001b[0m                     \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# dropout - make it harder to memorise data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                     \u001b[0mlosses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                     \u001b[0msgd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m                 )\n\u001b[1;32m     71\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Losses\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/spacy/lib/python3.7/site-packages/spacy/language.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, docs, golds, drop, sgd, losses, component_cfg)\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomponent_cfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"drop\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m             \u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgolds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_grads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m                 \u001b[0msgd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpipes.pyx\u001b[0m in \u001b[0;36mspacy.pipeline.pipes.EntityLinker.update\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [E188] Could not match the gold entity links to entities in the doc - make sure the gold EL data refers to valid results of the named entity recognizer in the `nlp` pipeline."
     ]
    }
   ],
   "source": [
    "def train_nel(kb_path, vocab_path=None, output_dir=None, n_iter=50):\n",
    "    \"\"\"Create a blank model with the specified vocab, set up the pipeline and train the entity linker.\n",
    "    The `vocab` should be the one used during creation of the KB.\"\"\"\n",
    "    vocab = Vocab().from_disk(vocab_path)\n",
    "    # create blank English model with correct vocab\n",
    "    nlp = spacy.blank(\"en\", vocab=vocab)\n",
    "    nlp.vocab.vectors.name = \"spacy_pretrained_vectors\"\n",
    "    print(\"Created blank 'en' model with vocab from '%s'\" % vocab_path)\n",
    "\n",
    "    # Add a sentencizer component. Alternatively, add a dependency parser for higher accuracy.\n",
    "    nlp.add_pipe(nlp.create_pipe('sentencizer'))\n",
    "\n",
    "    # Add a custom component to recognize \"Russ Cochran\" as an entity for the example training data.\n",
    "    # Note that in a realistic application, an actual NER algorithm should be used instead.\n",
    "    ruler = EntityRuler(nlp)\n",
    "    patterns = [{\"label\": \"PERSON\", \"pattern\": [{\"LOWER\": \"russ\"}, {\"LOWER\": \"cochran\"}]}]\n",
    "    ruler.add_patterns(patterns)\n",
    "    nlp.add_pipe(ruler)\n",
    "\n",
    "    # Create the Entity Linker component and add it to the pipeline.\n",
    "    if \"entity_linker\" not in nlp.pipe_names:\n",
    "        # use only the predicted EL score and not the prior probability (for demo purposes)\n",
    "        cfg = {\"incl_prior\": False}\n",
    "        entity_linker = nlp.create_pipe(\"entity_linker\", cfg)\n",
    "        kb = KnowledgeBase(vocab=nlp.vocab)\n",
    "        kb.load_bulk(kb_path)\n",
    "        print(\"Loaded Knowledge Base from '%s'\" % kb_path)\n",
    "        entity_linker.set_kb(kb)\n",
    "        nlp.add_pipe(entity_linker, last=True)\n",
    "\n",
    "    # Convert the texts to docs to make sure we have doc.ents set for the training examples.\n",
    "    # Also ensure that the annotated examples correspond to known identifiers in the knowlege base.\n",
    "    kb_ids = nlp.get_pipe(\"entity_linker\").kb.get_entity_strings()\n",
    "    TRAIN_DOCS = []\n",
    "    for text, annotation in TRAIN_DATA:\n",
    "        with nlp.disable_pipes(\"entity_linker\"):\n",
    "            doc = nlp(text)\n",
    "        annotation_clean = annotation\n",
    "        for offset, kb_id_dict in annotation[\"links\"].items():\n",
    "            new_dict = {}\n",
    "            for kb_id, value in kb_id_dict.items():\n",
    "                if kb_id in kb_ids:\n",
    "                    new_dict[kb_id] = value\n",
    "                else:\n",
    "                    print(\n",
    "                        \"Removed\", kb_id, \"from training because it is not in the KB.\"\n",
    "                    )\n",
    "            annotation_clean[\"links\"][offset] = new_dict\n",
    "        TRAIN_DOCS.append((doc, annotation_clean))\n",
    "\n",
    "    # get names of other pipes to disable them during training\n",
    "    pipe_exceptions = [\"entity_linker\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train entity linker\n",
    "        # reset and initialize the weights randomly\n",
    "        optimizer = nlp.begin_training()\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(TRAIN_DOCS)\n",
    "            losses = {}\n",
    "            # batch up the examples using spaCy's minibatch\n",
    "            batches = minibatch(TRAIN_DOCS, size=compounding(4.0, 32.0, 1.001))\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                nlp.update(\n",
    "                    texts,  # batch of texts\n",
    "                    annotations,  # batch of annotations\n",
    "                    drop=0.2,  # dropout - make it harder to memorise data\n",
    "                    losses=losses,\n",
    "                    sgd=optimizer,\n",
    "                )\n",
    "            print(itn, \"Losses\", losses)\n",
    "\n",
    "    # test the trained model\n",
    "    _apply_model(nlp)\n",
    "\n",
    "    # save model to output directory\n",
    "    if output_dir is not None:\n",
    "        output_dir = Path(output_dir)\n",
    "        if not output_dir.exists():\n",
    "            output_dir.mkdir()\n",
    "        nlp.to_disk(output_dir)\n",
    "        print()\n",
    "        print(\"Saved model to\", output_dir)\n",
    "\n",
    "        # test the saved model\n",
    "        print(\"Loading from\", output_dir)\n",
    "        nlp2 = spacy.load(output_dir)\n",
    "        _apply_model(nlp2)\n",
    "\n",
    "\n",
    "def _apply_model(nlp):\n",
    "    for text, annotation in TRAIN_DATA:\n",
    "        # apply the entity linker which will now make predictions for the 'Russ Cochran' entities\n",
    "        doc = nlp(text)\n",
    "        print()\n",
    "        print(\"Entities\", [(ent.text, ent.label_, ent.kb_id_) for ent in doc.ents])\n",
    "        print(\"Tokens\", [(t.text, t.ent_type_, t.ent_kb_id_) for t in doc])\n",
    "\n",
    "train_nel('./kb.out/kb', './kb.out/vocab', './nel.out')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
